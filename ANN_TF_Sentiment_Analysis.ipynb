{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np \n",
    "import random \n",
    "import pickle \n",
    "from collections import Counter \n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "hm_lines = 10000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(pos,neg): \n",
    "    lexicon = [] \n",
    "    for fi in [pos, neg]: \n",
    "        with open(fi, 'r') as f: \n",
    "            contents = f.readlines() \n",
    "            for l in contents[:hm_lines]: \n",
    "                all_words = word_tokenize(l.lower()) \n",
    "                lexicon += list(all_words) \n",
    " \n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon] \n",
    "    w_counts = Counter(lexicon) \n",
    " \n",
    "    #removing super common words like and, the, etc. \n",
    "    l2 =[] \n",
    "    for w in w_counts: \n",
    "        if 1000 > w_counts[w] > 50: \n",
    "            l2.append(w) \n",
    "    print(len(l2)) \n",
    "    return l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_handling(sample,lexicon,classification): \n",
    "    featureset = [] \n",
    " \n",
    "    with open(sample,'r') as f: \n",
    "        contents = f.readlines() \n",
    "        for l in contents[:hm_lines]: \n",
    "            current_words = word_tokenize(l.lower()) \n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words] \n",
    "            features = np.zeros(len(lexicon)) \n",
    "            for word in current_words: \n",
    "                if word.lower() in lexicon: \n",
    "                    index_value = lexicon.index(word.lower()) \n",
    "                    features[index_value] += 1 \n",
    " \n",
    "            features = list(features) \n",
    "            featureset.append([features,classification]) \n",
    " \n",
    "    return featureset \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_and_labels(pos,neg,test_size = 0.1): \n",
    "    lexicon = create_lexicon(pos,neg) \n",
    "    features = [] \n",
    "    features += sample_handling('pos.txt',lexicon,[1,0]) \n",
    "    features += sample_handling('neg.txt',lexicon,[0,1]) \n",
    "    random.shuffle(features) \n",
    "    features = np.array(features) \n",
    " \n",
    "    testing_size = int(test_size*len(features)) \n",
    " \n",
    "    train_x = list(features[:,0][:-testing_size]) \n",
    "    train_y = list(features[:,1][:-testing_size]) \n",
    "    test_x = list(features[:,0][-testing_size:]) \n",
    "    test_y = list(features[:,1][-testing_size:]) \n",
    " \n",
    "    return train_x,train_y,test_x,test_y \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__': \n",
    "#     train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt') \n",
    "#     # if you want to pickle this data: \n",
    "#     with open('/path/to/sentiment_set.pickle','wb') as f: \n",
    "#         pickle.dump([train_x,train_y,test_x,test_y],f)\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500 \n",
    "n_nodes_hl2 = 500 \n",
    "n_nodes_hl3 = 500 \n",
    " \n",
    "n_classes = 2 \n",
    "batch_size = 100 \n",
    "hm_epochs = 20 \n",
    "# height x width \n",
    " \n",
    "x = tf.placeholder('float') \n",
    "y = tf.placeholder('float') \n",
    " \n",
    " \n",
    "x = tf.placeholder('float') \n",
    "y = tf.placeholder('float') \n",
    " \n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1, \n",
    "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])), \n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))} \n",
    " \n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2, \n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))} \n",
    " \n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3, \n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))} \n",
    " \n",
    "output_layer = {'f_fum':None, \n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing changes \n",
    "def neural_network_model(data): \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias']) \n",
    "    l1 = tf.nn.relu(l1) \n",
    " \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias']) \n",
    "    l2 = tf.nn.relu(l2) \n",
    " \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['bias']) \n",
    "    l3 = tf.nn.relu(l3) \n",
    " \n",
    "    output = tf.matmul(l3, output_layer['weight']) + output_layer['bias'] \n",
    " \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x): \n",
    "    prediction = neural_network_model(x) \n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y)) \n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    " \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)) \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost) \n",
    " \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.initialize_all_variables()) \n",
    " \n",
    "        for epoch in range(hm_epochs): \n",
    "            epoch_loss = 0 \n",
    "            i = 0 \n",
    "            while i < len(train_x): \n",
    "                start = i \n",
    "                end = i + batch_size \n",
    "                batch_x = np.array(train_x[start:end]) \n",
    "                batch_y = np.array(train_y[start:end]) \n",
    " \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, \n",
    "                                                              y: batch_y}) \n",
    "                epoch_loss += c \n",
    "                i += batch_size \n",
    " \n",
    "            print('Epoch', epoch + 1, 'completed out of', hm_epochs, 'loss:', epoch_loss) \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)) \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float')) \n",
    " \n",
    "        print('Accuracy:', accuracy.eval({x: test_x, y: test_y})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 20 loss: 119297.46997070312\n",
      "Epoch 2 completed out of 20 loss: 62953.427001953125\n",
      "Epoch 3 completed out of 20 loss: 41594.07223510742\n",
      "Epoch 4 completed out of 20 loss: 28213.134155273438\n",
      "Epoch 5 completed out of 20 loss: 25497.9871673584\n",
      "Epoch 6 completed out of 20 loss: 21546.969131469727\n",
      "Epoch 7 completed out of 20 loss: 21650.610473632812\n",
      "Epoch 8 completed out of 20 loss: 15775.318248271942\n",
      "Epoch 9 completed out of 20 loss: 6917.495310783386\n",
      "Epoch 10 completed out of 20 loss: 3941.246570587158\n",
      "Epoch 11 completed out of 20 loss: 3447.722146987915\n",
      "Epoch 12 completed out of 20 loss: 2962.8135089874268\n",
      "Epoch 13 completed out of 20 loss: 3429.910904407501\n",
      "Epoch 14 completed out of 20 loss: 3864.7115926742554\n",
      "Epoch 15 completed out of 20 loss: 3581.4349880218506\n",
      "Epoch 16 completed out of 20 loss: 4252.712245464325\n",
      "Epoch 17 completed out of 20 loss: 3805.3816800117493\n",
      "Epoch 18 completed out of 20 loss: 3954.2941434383392\n",
      "Epoch 19 completed out of 20 loss: 3823.7727975845337\n",
      "Epoch 20 completed out of 20 loss: 5955.2413873672485\n",
      "Accuracy: 0.57912457\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
